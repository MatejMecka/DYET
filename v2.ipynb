{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4aaa47-1676-4d82-b6b1-382ead4c5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matus/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import FlattenObservation\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.distributions import DictDistribution\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_NUM_FUNCTIONS = 2\n",
    "MAX_INPUT = 3\n",
    "#MAX_STEPS = 6\n",
    "\n",
    "\n",
    "\n",
    "# class MyObservationWrapper(gym.ObservationWrapper):\n",
    "#     def __init__(self, env, max_num_functions, max_input):\n",
    "#         super(MyObservationWrapper, self).__init__(env)\n",
    "#         # self.observation_space = spaces.MultiDiscrete(\n",
    "#         #     [2 for _ in range(max_num_functions)] + \\\n",
    "#         #     [MAX_INPUT for _ in range(max_num_functions)] + \\\n",
    "#         #     np.array([max_num_functions, max_input, 3 for _ in range(max_num_functions)]).flatten().tolist()\n",
    "#         # )\n",
    "            \n",
    "#     def reverse_observation(self, observation):\n",
    "#         return {\n",
    "#             \"points\": observation[:7],\n",
    "#             \"features\": observation[7:]\n",
    "#         }\n",
    "    \n",
    "#     def observation(self, observation):\n",
    "#         return np.concatenate((observation[\"points\"], observation[\"features\"]), axis=0)\n",
    "    \n",
    "# class MyActionWrapper(gym.ActionWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super(MyActionWrapper, self).__init__(env)\n",
    "        \n",
    "#         fn_low = np.full((NUM_FUNCTIONS,), -10.0)\n",
    "#         fn_high = np.full((NUM_FUNCTIONS,), 10.0)\n",
    "        \n",
    "#         inputs_low = np.full((1,), 0.0)\n",
    "#         inputs_high = np.full((1,), MAX_INPUT)\n",
    "        \n",
    "#         self.action_space = spaces.Box(\n",
    "#             low=np.concatenate((fn_low, inputs_low), axis=0), \n",
    "#             high=np.concatenate((fn_high, inputs_high), axis=0),\n",
    "#             dtype=np.float32\n",
    "#         )\n",
    "        \n",
    "#         latent_dim = NUM_FUNCTIONS\n",
    "#         self.fn_net = nn.Linear(latent_dim, NUM_FUNCTIONS)\n",
    "        \n",
    "#     def action(self, action):\n",
    "#         #print(\"action:\", action)\n",
    "#         #print(\"action.size()\", len(action))\n",
    "#         #print(self.fn_net.weight)\n",
    "#         fn_logits = torch.tensor(action[:NUM_FUNCTIONS]) #self.fn_net(torch.tensor(action[:NUM_FUNCTIONS]))\n",
    "#         #print(\"fn_logits:\", fn_logits)\n",
    "#         #fn_index = Categorical(logits=fn_logits).sample().item()\n",
    "        \n",
    "#         fn_index = torch.argmax(Categorical(logits=fn_logits).probs).item()\n",
    "#         inputs = action[NUM_FUNCTIONS:]\n",
    "        \n",
    "#         #input_logits = torch.tensor(action[NUM_FUNCTIONS:])\n",
    "#         #input_index = Categorical(logits=input_logits).sample().item()\n",
    "        \n",
    "#         res = {\n",
    "#             \"function\": fn_index,\n",
    "#             \"input\": round(inputs[0])\n",
    "#         }\n",
    "        \n",
    "#         return res\n",
    "\n",
    "#     def reverse_action(self, action):\n",
    "#         fn_one_hot = np.zeros(NUM_FUNCTIONS)\n",
    "#         fn_i = action[\"function\"]\n",
    "#         fn_one_hot[fn_i] = 1.0\n",
    "        \n",
    "#         inputs = np.array([action[\"input\"],], dtype=np.float32)\n",
    "#         #input_one_hot = np.zeros(MAX_INPUT)\n",
    "#         #input_i = action[\"input\"]\n",
    "#         #input_one_hot[input_i] = 1.0\n",
    "        \n",
    "#         res = np.concatenate((fn_one_hot, inputs), axis=0)\n",
    "#         return res\n",
    "\n",
    "class MyObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MyObservationWrapper, self).__init__(env)\n",
    "        self.observation_space = spaces.MultiDiscrete([2 for _ in range(env.max_num_functions * 2)] + [env.max_input for _ in range(env.max_num_functions * 2)])\n",
    "            \n",
    "    def reverse_observation(self, observation):\n",
    "        return {\n",
    "            \"points\": observation[:env.max_num_functions * 2],\n",
    "            \"features\": observation[env.max_num_functions * 2:]\n",
    "        }\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        return np.concatenate((observation[\"points\"], observation[\"features\"]), axis=0)\n",
    "\n",
    "class MyActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MyActionWrapper, self).__init__(env)\n",
    "        self.action_space = spaces.MultiDiscrete([env.max_num_functions, env.max_input])\n",
    "        \n",
    "    def action(self, action):\n",
    "        #print(\"action:\", action)\n",
    "        #print(\"action.size()\", len(action))\n",
    "        return {\n",
    "            \"function\": action[0],\n",
    "            \"input\": action[1]\n",
    "        }\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return np.array([action[\"function\"], action[\"input\"]])\n",
    "    \n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3559f8e9-5c33-4484-afe5-899cacd04894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "#     def __init__(self, observation_space: gym.spaces.Dict):\n",
    "#         # We do not know features-dim here before going over all the items,\n",
    "#         # so put something dummy for now. PyTorch requires calling\n",
    "#         # nn.Module.__init__ before adding modules\n",
    "#         super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=1)\n",
    "\n",
    "#         extractors = {}\n",
    "\n",
    "#         total_concat_size = 0\n",
    "#         # We need to know size of the output of this extractor,\n",
    "#         # so go over all the spaces and compute output feature sizes\n",
    "#         # for key, subspace in observation_space.spaces.items():\n",
    "#         #     if key == \"image\":\n",
    "#         #         # We will just downsample one channel of the image by 4x4 and flatten.\n",
    "#         #         # Assume the image is single-channel (subspace.shape[0] == 0)\n",
    "#         #         extractors[key] = nn.Sequential(nn.MaxPool2d(4), nn.Flatten())\n",
    "#         #         total_concat_size += subspace.shape[1] // 4 * subspace.shape[2] // 4\n",
    "#         #     elif key == \"vector\":\n",
    "#         #         # Run through a simple MLP\n",
    "#         #         extractors[key] = nn.Linear(subspace.shape[0], 16)\n",
    "#         #         total_concat_size += 16\n",
    "\n",
    "#         self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "#         # Update the features dim manually\n",
    "#         self._features_dim = total_concat_size\n",
    "\n",
    "#     def forward(self, observations) -> torch.Tensor:\n",
    "#         print(\"CustomCombinedExtractor.forward\")\n",
    "#         print(\"observations.size():\", observations.size())\n",
    "#         print(\"observations:\", observations)\n",
    "#         encoded_tensor_list = []\n",
    "\n",
    "#         # self.extractors contain nn.Modules that do all the processing.\n",
    "#         for key, extractor in self.extractors.items():\n",
    "#             encoded_tensor_list.append(extractor(observations[key]))\n",
    "#         # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "#         return torch.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "class CustomExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Feature extract that flatten the input.\n",
    "    Used as a placeholder when feature extraction is not needed.\n",
    "\n",
    "    :param observation_space:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.Space):\n",
    "        #self.d_out = 64\n",
    "        super().__init__(observation_space, features_dim=120)#self.d_out)\n",
    "        #self.features_d = MAX_INPUT * 2\n",
    "        #self.action_history_d = 2 * 2\n",
    "        #self.d_model = 64\n",
    "        \n",
    "        \n",
    "#         self.fc_features_q = nn.Linear(self.features_d, self.d_model)\n",
    "#         self.fc_action_history_k = nn.Linear(self.action_history_d, self.d_model)\n",
    "#         self.fc_action_history_v = nn.Linear(self.action_history_d, self.d_model)\n",
    "        \n",
    "#         self.multihead_attention = MultiHeadAttention(\n",
    "#             n_head=8, \n",
    "#             d_model=self.d_model, \n",
    "#             d_k=64, \n",
    "#             d_v=64, \n",
    "#             dropout=0.0\n",
    "#         )\n",
    "        \n",
    "#         self.fc_out = nn.Linear(self.d_model, self.d_out)\n",
    "\n",
    "        #self.fc1 = nn.Linear(24, self.d_model)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return observations\n",
    "#         points = observations[:, :2 * 2 * MAX_NUM_FUNCTIONS].reshape(-1, self.action_history_d).unsqueeze(0)\n",
    "#         features = observations[:, 2 * 2 * MAX_NUM_FUNCTIONS:].reshape(-1, self.features_d).unsqueeze(0)\n",
    "#         #print(\"points:\", points, points.size())\n",
    "#         #print(\"features:\", features, features.size())\n",
    "        \n",
    "#         x = torch.cat([features, points], dim=2)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = torch.max(x, dim=1).values\n",
    "        \n",
    "#         #print(\"x:\", x, x.size())\n",
    "        \n",
    "#         #print(observations.size(), x.size())\n",
    "        \n",
    "        #return x\n",
    "        \n",
    "#         #print(\"CustomExtractor-observations:\", observations, observations.size())\n",
    "        \n",
    "#         points = observations[:, :2 * 2 * MAX_NUM_FUNCTIONS].reshape(-1, self.action_history_d).unsqueeze(0)\n",
    "#         features = observations[:, 2 * 2 * MAX_NUM_FUNCTIONS:].reshape(-1, self.features_d).unsqueeze(0)\n",
    "        \n",
    "#         #print(\"points:\", points, points.size())\n",
    "#         #print(\"features:\", features, features.size())\n",
    "#         #features = observations[\"features\"].reshape(1, -1, self.features_d)\n",
    "#         #action_history = observations[\"points\"].reshape(1, -1, self.action_history_d)\n",
    "        \n",
    "#         #print(f\"features: {features.size()}, action_history: {action_history.size()}\")\n",
    "        \n",
    "#         f_q = F.relu(self.fc_features_q(features))\n",
    "#         ah_k = F.relu(self.fc_action_history_k(points))\n",
    "#         ah_v = F.relu(self.fc_action_history_v(points))\n",
    "        \n",
    "#         #print(f\"f_q: {f_q.size()}, ah_k: {ah_k.size()}, ah_v: {ah_v.size()}\")\n",
    "        \n",
    "#         f_mha, _w = self.multihead_attention(f_q, ah_k, ah_v)\n",
    "        \n",
    "#         #print(\"f_mha:\", f_mha)\n",
    "#         #print(\"f_mha.size():\", f_mha.size())\n",
    "        \n",
    "#         f_out = F.relu(self.fc_out(f_mha)).sum(1)\n",
    "        \n",
    "#         #print(\"f_out:\", f_out)\n",
    "#         #print(\"f_out.size():\", f_out.size())\n",
    "        \n",
    "#         return f_out\n",
    "\n",
    "class CustomModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModule, self).__init__()\n",
    "        self.d_model = 64\n",
    "        self.features_d = MAX_INPUT * 2\n",
    "        self.action_history_d = 2 * 2\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        #return observations\n",
    "        batch_dim = observations.size()[0]\n",
    "    \n",
    "        points = observations[:, :2 * 2 * MAX_NUM_FUNCTIONS].reshape(batch_dim, -1, self.action_history_d)\n",
    "        features = observations[:, 2 * 2 * MAX_NUM_FUNCTIONS:].reshape(batch_dim, -1, self.features_d)\n",
    "        \n",
    "        #print(\"points.size:\", points.size())\n",
    "        #print(\"features.size:\", features.size())\n",
    "        \n",
    "        \n",
    "        x = torch.cat([features, points], dim=2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.max(x, dim=1).values\n",
    "        #x = torch.mean(x, dim=1)\n",
    "        \n",
    "        # if observations.size()[0] > 1:\n",
    "        #     print(\"obs, x:\", observations.size(), x.size())\n",
    "        #     print(\"points:\", points, points.size())\n",
    "        #     print(\"features:\", features, features.size())\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the feature extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 128,\n",
    "        last_layer_dim_vf: int = 128,\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        \n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            CustomModule(),\n",
    "            nn.Linear(64, last_layer_dim_pi), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_pi, last_layer_dim_pi), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_pi, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            CustomModule(),\n",
    "            nn.Linear(64, last_layer_dim_vf), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_vf, last_layer_dim_vf), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_pi, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        \n",
    "        p = self.policy_net(features)\n",
    "        v = self.value_net(features)\n",
    "        #print(\"forward:\", features.size(), p.size(), v.size())\n",
    "        return p, v\n",
    "\n",
    "    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        p = self.policy_net(features)\n",
    "        #print(\"p:\", features.size(), p.size())\n",
    "        return p\n",
    "\n",
    "    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        v = self.value_net(features)\n",
    "        #print(\"v:\", features.size(), v.size())\n",
    "        return v\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bfe6c9-8c45-44d5-b72c-1df7829ac8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matus/Documents/soroban/simple-contract-generator/baselines3/stable_baselines3/common/env_checker.py:81: UserWarning: The action space is not based off a numpy array. Typically this means it's either a Dict or Tuple space. This type of action space is currently not supported by Stable Baselines 3. You should try to flatten the action using a wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class DummyContractV2:\n",
    "    def __init__(self, num_functions, features):\n",
    "        self.num_functions = num_functions\n",
    "        self.points = [0 for i in range(2 * num_functions)] #coverage\n",
    "        \n",
    "        assert(len(features) == 2 * self.num_functions)\n",
    "        self.features = features\n",
    "    \n",
    "    def get_points_sum(self):\n",
    "        return sum(self.points) #np.array(self.points).sum()\n",
    "        \n",
    "    def call(self, fn_i, x):\n",
    "        prev_score = self.get_points_sum()\n",
    "        \n",
    "        if fn_i < self.num_functions:\n",
    "             \n",
    "            if x == self.features[2 * fn_i]:\n",
    "                self.points[2 * fn_i] = 1\n",
    "\n",
    "            if x == self.features[2 * fn_i + 1]:\n",
    "                self.points[2 * fn_i + 1] = 1\n",
    "            \n",
    "        score = self.get_points_sum()\n",
    "        reward = score - prev_score\n",
    "        reward = -1 if reward == 0 else reward\n",
    "        return reward\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, max_num_functions, max_input):\n",
    "        super(MyEnv, self).__init__()\n",
    "        self.contract = None\n",
    "        self.num_steps = None\n",
    "        self.action_history = None\n",
    "        \n",
    "        self.max_num_functions = max_num_functions\n",
    "        self.max_input = max_input\n",
    "        self.action_history_size = 2 * max_num_functions\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"points\": spaces.MultiBinary(2 * max_num_functions), \n",
    "            \"features\": spaces.MultiDiscrete([max_input for i in range(2 * max_num_functions)]),\n",
    "            #spaces.Box(low=0.0, high=max_input, shape=(2 * max_num_functions,)),\n",
    "            #spaces.MultiDiscrete([max_input for i in range(2 * max_num_functions)]),\n",
    "            # \"action_history\": spaces.Box(\n",
    "            #     low=-1.0, \n",
    "            #     high=max_input, \n",
    "            #     shape=(self.action_history_size, 3)\n",
    "            # )\n",
    "        })\n",
    "        \n",
    "        self.action_space = spaces.Dict({\n",
    "            \"function\": spaces.Discrete(max_num_functions), \n",
    "            \"input\": spaces.Discrete(max_input) #spaces.Box(low=0.0, high=MAX_INPUT, shape=(1,))\n",
    "        })\n",
    "    \n",
    "    def create_observation(self):\n",
    "        points = self.contract.points\n",
    "        features = self.contract.features\n",
    "        actions = self.action_history\n",
    "        \n",
    "        obs_points = np.zeros(self.observation_space[\"points\"].shape, dtype=np.int8)\n",
    "        obs_features = np.zeros(self.observation_space[\"features\"].shape, dtype=np.float32)\n",
    "        #obs_action_history = np.zeros(self.observation_space[\"action_history\"].shape, dtype=np.float32)\n",
    "        \n",
    "        obs_points[0:len(points)] = points\n",
    "        obs_features[0:len(features)] = features\n",
    "        \n",
    "        #if len(self.action_history) > 0:\n",
    "        #    obs_action_history[0:len(self.action_history), :] = self.action_history\n",
    "            #print(obs_action_history)\n",
    "        \n",
    "        return {\n",
    "            \"points\": obs_points, \n",
    "            \"features\": obs_features,\n",
    "            #\"action_history\": obs_action_history\n",
    "        }\n",
    "        \n",
    "    def reset(self):\n",
    "        num_functions = random.randint(2, self.max_num_functions)\n",
    "        \n",
    "        features = self.observation_space[\"features\"].sample().tolist()\n",
    "        \n",
    "        # print(features, len(features))\n",
    "        # print(num_functions)\n",
    "        # print(features[:2 * num_functions])\n",
    "        \n",
    "        self.contract = DummyContractV2(num_functions, features[:2 * num_functions])\n",
    "        self.num_steps = 0\n",
    "        self.action_history = []\n",
    "        \n",
    "        return self.create_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        #print(action)\n",
    "        self.num_steps += 1\n",
    "        fn_index, fn_input = action[\"function\"], action[\"input\"]\n",
    "        \n",
    "        prev_score = self.contract.get_points_sum()\n",
    "        self.contract.call(fn_index, fn_input)\n",
    "        score = self.contract.get_points_sum()\n",
    "        reward = score - prev_score\n",
    "        reward = -1 if reward == 0 else reward\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        max_score = self.contract.num_functions * 2\n",
    "        \n",
    "        done = self.num_steps == max_score or score == max_score\n",
    "                \n",
    "        self.action_history.append([fn_index, fn_input, reward])\n",
    "        \n",
    "        observation = self.create_observation()\n",
    "        \n",
    "        reward /= max_score\n",
    "        \n",
    "        #print(f\"reward: {reward}, score: {score}, max_score: {max_score}, num_steps: {self.num_steps}\")\n",
    "\n",
    "        return observation, reward, done, info\n",
    "        \n",
    "    def render(self, mode='console'):        \n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "env = MyEnv(max_num_functions=MAX_NUM_FUNCTIONS, max_input=MAX_INPUT)\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518ecde4-47fb-4d60-b59c-d11fd63206f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = MyObservationWrapper(MyActionWrapper(MyEnv(max_num_functions=MAX_NUM_FUNCTIONS, max_input=MAX_INPUT)))\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "policy_kwargs = {\n",
    "    \"features_extractor_class\": CustomExtractor,\n",
    "    #features_extractor_kwargs=dict(features_dim=128),\n",
    "}\n",
    "\n",
    "#policy_kwargs = dict(net_arch=[64, 64])\n",
    "\n",
    "#model = PPO(MlpPolicy, env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99f9ab2-be26-407c-9d89-4b77dc379727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNetwork(\n",
       "  (policy_net): Sequential(\n",
       "    (0): CustomModule(\n",
       "      (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (value_net): Sequential(\n",
       "    (0): CustomModule(\n",
       "      (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.mlp_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dde4202-a867-4334-be60-5298fa188a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomExtractor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.features_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053345cb-37e8-46b1-a7de-dc68505937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.env.envs[0].contract.num_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c31a415b-1a75-4923-89bd-1383d4177ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-0.72 +/- 0.36\n",
      "CPU times: user 25.9 ms, sys: 23.8 ms, total: 49.8 ms\n",
      "Wall time: 45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d25c68-6889-4557-bf83-04f6b798f078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e92f2c2c-a65c-44be-9fb4-4b5b8d8e6209",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 10000\n",
      "Best mean reward: -inf - Last mean reward per episode: 0.17\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 0.17 - Last mean reward per episode: 0.36\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 0.36 - Last mean reward per episode: 0.39\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 0.39 - Last mean reward per episode: 0.49\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 0.49 - Last mean reward per episode: 0.71\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 0.71 - Last mean reward per episode: 0.73\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 0.73 - Last mean reward per episode: 0.82\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 0.82 - Last mean reward per episode: 0.85\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.77\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 0.85 - Last mean reward per episode: 0.91\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.92\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.74\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.94\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.71\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.52\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.70\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 510000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 520000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 530000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 540000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 550000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 560000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 570000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 580000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 590000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 600000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 610000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 620000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.96\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 630000\n",
      "Best mean reward: 0.96 - Last mean reward per episode: 0.97\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 640000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.69\n",
      "Num timesteps: 650000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 660000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 670000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 680000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 690000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 700000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 710000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 720000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 730000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 740000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 750000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 760000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 770000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 780000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 790000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 800000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 810000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 820000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 830000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 840000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 850000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.56\n",
      "Num timesteps: 860000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 870000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 880000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 890000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 900000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 910000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.78\n",
      "Num timesteps: 920000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 930000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 940000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 950000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 960000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 970000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 980000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 990000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1010000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 1020000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1030000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1040000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1050000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1060000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 1070000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1080000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1090000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1100000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1110000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1120000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1130000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1140000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1150000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 1160000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 1170000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1180000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 1190000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1200000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1210000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 1220000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1230000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1240000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1250000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1260000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1270000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1280000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1290000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1300000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.63\n",
      "Num timesteps: 1310000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 1320000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1330000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 1340000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1350000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1360000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.72\n",
      "Num timesteps: 1370000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 1380000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 1390000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.66\n",
      "Num timesteps: 1400000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1410000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1420000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1430000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.74\n",
      "Num timesteps: 1440000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 1450000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1460000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1470000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1480000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1490000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 1500000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1510000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1520000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 1530000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1540000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 1550000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1560000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1570000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1580000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1590000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1600000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 1610000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 1620000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1630000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1640000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 1650000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1660000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1670000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1680000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1690000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 1700000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 1710000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 1720000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1730000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 1740000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 1750000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1760000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1770000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1780000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 1790000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 1800000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 1810000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1820000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 1830000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1840000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1850000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 1860000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1870000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 1880000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 1890000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.26\n",
      "Num timesteps: 1900000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 1910000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 1920000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 1930000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 1940000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1950000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 1960000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 1970000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 1980000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 1990000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 2000000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2010000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2020000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2030000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.66\n",
      "Num timesteps: 2040000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 2050000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.98\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 2060000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2070000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 2080000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2090000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2100000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 2110000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2120000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2130000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2140000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2150000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2160000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2170000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 2180000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.78\n",
      "Num timesteps: 2190000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 2200000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2210000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2220000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2230000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2240000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2250000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 2260000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2270000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2280000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2290000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 2300000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2310000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 2320000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.52\n",
      "Num timesteps: 2330000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2340000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2350000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2360000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2370000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 2380000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 2390000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2400000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2410000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2420000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.74\n",
      "Num timesteps: 2430000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2440000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2450000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2460000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2470000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2480000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 2490000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2500000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2510000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 2520000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 2530000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2540000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2550000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 2560000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2570000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2580000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.78\n",
      "Num timesteps: 2590000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2600000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2610000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2620000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2630000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2640000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2650000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 2660000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 2670000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 2680000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 2690000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2700000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 2710000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2720000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 2730000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 2740000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 2750000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2760000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2770000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2780000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2790000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 2800000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 2810000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2820000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 2830000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2840000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 2850000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2860000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2870000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 2880000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2890000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 2900000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 2910000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2920000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 2930000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 2940000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 2950000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2960000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 2970000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 2980000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 2990000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3000000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3010000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3020000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 3030000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3040000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3050000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 3060000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3070000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3080000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3090000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3100000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.64\n",
      "Num timesteps: 3110000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3120000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3130000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.30\n",
      "Num timesteps: 3140000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3150000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3160000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3170000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 3180000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3190000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3200000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 3210000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3220000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3230000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3240000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3250000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3260000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3270000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3280000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 3290000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 3300000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3310000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3320000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3330000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 3340000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3350000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3360000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3370000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.98\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 3380000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 3390000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3400000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3410000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3420000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3430000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3440000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3450000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 3460000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3470000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3480000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3490000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3500000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3510000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3520000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3530000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 3540000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3550000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3560000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3570000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 3580000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 3590000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 3600000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3610000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3620000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3630000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3640000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3650000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3660000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3670000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3680000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3690000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3700000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 3710000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 3720000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.99\n",
      "Saving new best model to ./tmp/gym/best_model.zip\n",
      "Num timesteps: 3730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 3760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.39\n",
      "Num timesteps: 3810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 3870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.74\n",
      "Num timesteps: 3910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 3920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 3930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 3940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 3950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 3960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 3970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 3980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 3990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 4070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 4080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 4120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 4140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 4160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 4190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 4230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.76\n",
      "Num timesteps: 4240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 4260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 4280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 4390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 4420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 4430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 4440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 4460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 4480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 4490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 4530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 4550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 4560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 4630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 4690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 4710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.72\n",
      "Num timesteps: 4780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 4790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.73\n",
      "Num timesteps: 4840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 4870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 4920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 4930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 4940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 4960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 4970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 4980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 4990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 5050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 5110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 5120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 5240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 5250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 5310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 5320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 5330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 5340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 5350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 5420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 5440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 5460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 5500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 5510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 5540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 5550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 5590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 5600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 5630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 5680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 5720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 5740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 5750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 5770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 5780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 5800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 5840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 5850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 5870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 5890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 5930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 5940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 5950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 5960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 5970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 5990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 6070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 6100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 6140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 6180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 6210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 6240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 6270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 6290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 6300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 6340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 6360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 6510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 6520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 6530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 6550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 6560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 6660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 6760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 6790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 6850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 6870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 6880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 6890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 6910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 6940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 6950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 6960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 6970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 6980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 6990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 7080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.73\n",
      "Num timesteps: 7090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 7100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 7110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 7160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 7240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 7320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 7390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.60\n",
      "Num timesteps: 7470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 7490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 7560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 7630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 7710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 7730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 7750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 7790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 7820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 7830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 7840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 7870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 7880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 7890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 7910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 7940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 7950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 7960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 7970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 7990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 8020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 8050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 8080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 8170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 8210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 8260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 8320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 8340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 8370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 8420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 8490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 8540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 8560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 8590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 8660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 8680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 8720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 8740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 8770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 8790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.84\n",
      "Num timesteps: 8800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 8810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 8830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 8900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 8910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 8930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 8940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "Num timesteps: 8950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 8960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 8970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 8980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 8990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 9000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9010000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9020000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 9030000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9040000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 9050000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9060000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9070000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9080000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9090000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9100000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 9110000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9120000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9130000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 9140000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9150000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9160000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9170000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9180000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 9190000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9200000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9210000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 9220000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 9230000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 9240000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9250000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9260000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9270000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 9280000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9290000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9300000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9310000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 9320000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.80\n",
      "Num timesteps: 9330000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9340000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9350000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "Num timesteps: 9360000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9370000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.96\n",
      "Num timesteps: 9380000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9390000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9400000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 9410000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9420000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9430000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9440000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9450000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9460000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.79\n",
      "Num timesteps: 9470000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9480000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9490000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9500000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9510000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 9520000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 9530000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.83\n",
      "Num timesteps: 9540000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9550000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9560000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9570000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 9580000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 9590000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9600000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9610000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9620000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 9630000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9640000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9650000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 9660000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9670000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9680000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9690000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 9700000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9710000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 9720000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9730000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9740000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9750000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9760000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 9770000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9780000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.82\n",
      "Num timesteps: 9790000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.91\n",
      "Num timesteps: 9800000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9810000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9820000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.85\n",
      "Num timesteps: 9830000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.88\n",
      "Num timesteps: 9840000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9850000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9860000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 9870000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.87\n",
      "Num timesteps: 9880000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9890000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.56\n",
      "Num timesteps: 9900000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 9910000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9920000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 9930000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 9940000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9950000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.89\n",
      "Num timesteps: 9960000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.86\n",
      "Num timesteps: 9970000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9980000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.94\n",
      "Num timesteps: 9990000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.93\n",
      "Num timesteps: 10000000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.81\n",
      "mean_reward:0.73 +/- 0.65\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000000, callback=callback)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5a07324d-156e-4fdf-bb6c-131e352a9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# for i in range(100):\n",
    "#     env = MyActionWrapper(MyObservationWrapper(MyEnv()))\n",
    "#     obs = env.reset()\n",
    "#     prints = []\n",
    "\n",
    "#     for j in range(50):\n",
    "#         action, _ = model.predict(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "        \n",
    "#         prints.append([action, obs, reward, done])\n",
    "         \n",
    "#         if done:\n",
    "#             if j > 6:\n",
    "#                 pprint(prints)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cea50217-6040-477e-9edb-712ab2bfa118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:0.75 +/- 0.63\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c89ec6a6-84d0-421b-9db9-c8d4461fcd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], 1e5, results_plotter.X_TIMESTEPS, \"Table name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "54cbb7f0-5c0a-495a-8cd9-0e5c7a476143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=100)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1b9e9682-4625-42d8-9775-8140801e1bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba069f75-32cc-46ed-9976-c46053eff79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e84385-d0b0-4314-b6fb-2db2301d97ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4912826-0f64-4385-aef0-522ef799b4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'points': array([0, 0, 0, 0], dtype=int8),\n",
       " 'features': array([2., 2., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MyEnv(max_num_functions=2, max_input=3)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc8c7c-214c-4775-8c7e-87dda0003cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "env.step({\"function\": 0, \"input\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfcb53-8c1a-4bb2-8ddf-eea0208d6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyContractV2(num_fns, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b4d3b611-b89b-40b4-8a3c-966fd4ce0ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'action_history',\n",
       " 'action_history_size',\n",
       " 'action_space',\n",
       " 'close',\n",
       " 'contract',\n",
       " 'create_observation',\n",
       " 'max_input',\n",
       " 'max_num_functions',\n",
       " 'metadata',\n",
       " 'num_steps',\n",
       " 'observation_space',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_range',\n",
       " 'seed',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'unwrapped']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b352c449-7ced-4ce5-9655-47b528c0dc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Box',\n",
       " 'Dict',\n",
       " 'Discrete',\n",
       " 'MultiBinary',\n",
       " 'MultiDiscrete',\n",
       " 'Space',\n",
       " 'Tuple',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'box',\n",
       " 'dict',\n",
       " 'discrete',\n",
       " 'flatdim',\n",
       " 'flatten',\n",
       " 'flatten_space',\n",
       " 'multi_binary',\n",
       " 'multi_discrete',\n",
       " 'space',\n",
       " 'tuple',\n",
       " 'unflatten',\n",
       " 'utils']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gym.spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8eb7d60-a0fe-41b0-b3a7-35042c056451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2, 3) + (4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4820f5bf-ed87-49d0-9a47-a2fd222a424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = spaces.Box(\n",
    "    low=0.0, \n",
    "    high=1.0, \n",
    "    shape=(10, 5)\n",
    ")\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c8ea94-d309-428f-b8e8-8361b217d60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('action_history',\n",
       "              array([[1.8066711 , 2.9890919 , 6.019261  , 4.67686   , 3.2270215 ],\n",
       "                     [3.9573112 , 3.1858263 , 2.4332523 , 0.42170376, 7.0352054 ],\n",
       "                     [2.2451136 , 1.404828  , 2.064434  , 7.6609116 , 1.3292111 ],\n",
       "                     [0.06347949, 2.0168817 , 1.0336297 , 6.1379805 , 5.278453  ],\n",
       "                     [9.80183   , 7.031457  , 1.289134  , 1.4286542 , 9.106412  ],\n",
       "                     [9.737788  , 8.657428  , 6.9548597 , 0.799518  , 6.0859194 ],\n",
       "                     [3.4112654 , 9.002056  , 6.0207267 , 3.6499624 , 7.531077  ],\n",
       "                     [5.853317  , 0.7257287 , 7.375656  , 0.74590886, 9.674888  ],\n",
       "                     [9.841174  , 7.3044086 , 5.6559286 , 5.5741415 , 8.845847  ],\n",
       "                     [4.927478  , 6.6615987 , 8.577588  , 3.1652172 , 7.383477  ],\n",
       "                     [4.61448   , 9.162831  , 4.26521   , 0.9749578 , 0.17799926],\n",
       "                     [9.681989  , 9.029232  , 5.3188777 , 2.6579444 , 3.036686  ],\n",
       "                     [4.287898  , 4.8447347 , 4.6379495 , 0.19437547, 8.292537  ],\n",
       "                     [8.379176  , 8.508844  , 0.70354956, 1.166954  , 2.7029347 ],\n",
       "                     [3.8518283 , 2.2991397 , 0.964223  , 2.6104145 , 3.11786   ]],\n",
       "                    dtype=float32)),\n",
       "             ('features', array([8, 5, 5, 2, 2, 7, 5, 9, 5, 9])),\n",
       "             ('points', array([1, 1, 1, 1, 1], dtype=int8))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaces.Dict({\n",
    "    \"points\": spaces.MultiBinary(5), \n",
    "    \"features\": spaces.MultiDiscrete([10 for i in range(2 * 5)]),\n",
    "    \"action_history\": spaces.Box(\n",
    "        low=0.0, \n",
    "        high=10, \n",
    "        shape=(15, 5)\n",
    "    )\n",
    "}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b14c40d7-8f54-4966-ae98-3556f0c06441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.zeros(10)\n",
    "o = np.ones(3)\n",
    "z[0:3] = [1, 1, 1]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26fe99b4-ff2a-418d-a7eb-f5337e06670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d079e4a9-d6de-4fb9-b8d1-ab93e7f87bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[3,4]]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9a18505-321a-47da-9c6a-0165ae1e0db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a2e7dbc345d1a879e64cfaddfab198d3655d21b41766e94debd2b1a2b484791a'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stellar_sdk import Keypair\n",
    "\n",
    "Keypair.from_secret(\"SDTARUMXE5JTFB7MI3CIFOLXGBAJIGRZ4MSTLOTVJ2XIH33EBKCANQDL\").raw_public_key().hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3114f92-5185-4706-93ba-f0cf1985f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(1, 7, 3)\n",
    "t[0, 1] = t[0, 0]\n",
    "m = nn.Linear(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4675daef-18cb-4422-b18e-91d90d6ff714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2254,  0.1553,  0.5132],\n",
       "        [-0.2180, -0.4014,  0.0298],\n",
       "        [-0.3472,  0.4954, -0.4684],\n",
       "        [-0.0492,  0.1083,  0.4056],\n",
       "        [-0.4402, -0.3269, -0.4383]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e7cfb01-c776-452e-850d-5464b743a3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1954,  0.5282,  0.6225],\n",
       "         [ 0.1954,  0.5282,  0.6225],\n",
       "         [ 1.0196,  0.8855,  0.9625],\n",
       "         [ 0.4920, -0.3205, -0.4694],\n",
       "         [ 2.7847, -0.7034,  0.3286],\n",
       "         [-0.6364,  1.5823, -0.6974],\n",
       "         [ 0.7163, -2.5230, -1.0391]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7658e8b8-7343-496d-8855-667634f5c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2686, -0.0256,  0.8412,  0.5502,  0.0189],\n",
       "         [-1.2686, -0.0256,  0.8412,  0.5502,  0.0189],\n",
       "         [-1.0182,  0.0411,  0.0860,  0.5227,  0.4728],\n",
       "         [ 0.5613,  0.5726, -0.6034, -0.3849,  0.4606],\n",
       "         [-1.2339,  1.2220,  1.1933, -0.6740, -0.8359],\n",
       "         [ 0.7820,  1.0854,  0.0727, -1.0384, -0.2889],\n",
       "         [-0.7914,  0.1989,  0.4601,  0.2434,  0.1039],\n",
       "         [-1.1928,  0.4442,  0.3060,  0.1719,  0.1502],\n",
       "         [-0.5270,  0.2988, -0.3139,  0.1834,  0.5527],\n",
       "         [-1.0877,  1.9598,  0.3778, -1.2810, -0.6510]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d94d8be3-3888-469d-8931-9998ff12f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(1, 7, 4)\n",
    "k = torch.randn(1, 10, 5)\n",
    "v = torch.randn(1, 10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "077ab76c-f5db-40bf-96ed-be102605cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = torch.nn.MultiheadAttention(\n",
    "    embed_dim=4, \n",
    "    num_heads=2, \n",
    "    dropout=0.0, \n",
    "    bias=True, \n",
    "    add_bias_kv=False, \n",
    "    add_zero_attn=False, \n",
    "    kdim=5, \n",
    "    vdim=6, \n",
    "    batch_first=True, \n",
    "    device=None, \n",
    "    dtype=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "37b51c88-26ee-4d97-b979-448e2296a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_padding_mask = torch.tensor([[False, True, True, True, True, True, True, True, True, True]])\n",
    "attn_output, attn_output_weights = mha(q, k, v, key_padding_mask=key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f184b579-8f8f-459d-ac88-4025b237fb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223],\n",
       "          [-1.1546,  0.1890, -1.1885, -1.3223]]], grad_fn=<TransposeBackward0>),\n",
       " torch.Size([1, 7, 4]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output, attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d89c04b9-6445-493f-a2a9-b27889abf82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0486, -0.6342, -1.4413, -1.5537],\n",
       "         [-1.5929,  0.2420, -0.6904, -0.3219],\n",
       "         [-0.2475,  1.7527,  1.1416, -0.6094],\n",
       "         [-0.7674,  0.9008, -0.1188,  0.4237],\n",
       "         [-1.1797,  0.3144,  1.3335,  0.2820],\n",
       "         [-0.9853,  0.2039, -2.4896,  0.0520],\n",
       "         [ 0.9821, -0.1935, -0.2306,  0.4600]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6861c807-79a7-486f-a203-1f863d62dd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 7])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc854527-57e4-4af6-ad67-8d49f270a020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5215,  0.3591, -0.1606, -0.3773],\n",
       "        [ 0.0117,  0.0540,  0.5056, -0.4940],\n",
       "        [ 0.8076, -0.5054,  0.3916, -0.1781],\n",
       "        [ 0.4067, -0.3117, -0.7222,  0.6166]], requires_grad=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.q_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1db9d044-9f93-49df-8462-a3244e77403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-7.4075e-01,  2.7383e-02,  3.3585e-01,  6.1743e-01,  5.2101e-01],\n",
       "        [ 1.3673e-04, -8.0710e-01,  6.5254e-01,  2.0762e-01,  4.6144e-01],\n",
       "        [-8.1145e-01,  7.8739e-01,  7.2257e-01,  6.5952e-02,  3.3717e-01],\n",
       "        [ 7.9695e-01,  1.3206e-01, -6.0681e-01,  8.8172e-02,  7.2054e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.k_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "da0d4a41-8ead-4ddb-8287-a3a9f6d9296d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7285, -0.6278, -0.1503, -0.1630, -0.1751, -0.7286],\n",
       "        [ 0.3590, -0.1418, -0.2774, -0.2966,  0.0678,  0.1418],\n",
       "        [-0.2482, -0.4547, -0.0364, -0.6998,  0.4701,  0.1494],\n",
       "        [ 0.5341, -0.0446,  0.6826, -0.1015, -0.2243,  0.6125]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8d364402-a73c-4224-aab5-22ef851441f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.head_dim, mha.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cf3660-2c8a-48a8-a0c4-f82c04b08fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(7, 4).unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "18b105b4-fb79-451c-8cb4-25761a5dab00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2009eea8-0497-4ba7-a424-642b3cc7570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_attention = MultiHeadAttention(n_head=5, d_model=5, d_k=5, d_v=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "69acfd01-d617-48ef-9832-5b1fa9acf556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5455, -0.7632, -0.3622,  1.6721, -1.0922],\n",
       "         [ 1.3012, -1.4202, -0.5786, -0.2478,  0.9454],\n",
       "         [ 0.0426, -0.3696,  1.1598,  0.8425, -1.6753],\n",
       "         [ 1.6201,  0.0351, -0.1981, -1.5265,  0.0694],\n",
       "         [-1.4236, -0.7666,  1.2259,  0.0251,  0.9393],\n",
       "         [-1.7280,  1.3086, -0.2614,  0.3695,  0.3112],\n",
       "         [ 1.0404,  0.4389,  0.8355, -1.5745, -0.7402]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(1, 7, 5)\n",
    "k = torch.randn(1, 10, 5)\n",
    "v = torch.randn(1, 10, 5)\n",
    "\n",
    "o, w = multi_head_attention(q, k, v)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f15485d9-1d51-417c-a5bd-8078508a0c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4014,  1.8224, -0.3020,  1.5385, -0.8151, -0.5202,  1.3501],\n",
       "         [-0.7472, -1.3870, -0.5662,  0.2226, -0.3619,  1.5350,  0.8623],\n",
       "         [-0.8376, -0.4755,  0.6338, -0.2226,  0.2657,  0.4583,  1.0264],\n",
       "         [ 0.0283,  0.0478,  0.4421, -1.3932, -0.1969,  0.9932, -1.6617],\n",
       "         [-1.1351,  1.3394, -2.0995,  0.0468,  0.0636,  0.5746, -0.7659]]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5dd61e4b-2301-44c8-9184-9d268c5f608d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 5])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.ones(1, 7, 5)\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "14e68869-95b1-4e94-b304-e62d3d5ca601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7., 7., 7., 7.]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = torch.sum(q, dim=1)\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3eb0ea1f-7ed2-419d-aa0b-6f6ebc4f07a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f40d9-9945-46d0-b70a-05646d52d370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "42d8e84d-9b88-4cb0-88f7-0a597f29a5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 1\n",
    "torch.randn(b, 20).reshape(b, -1, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec87a9d-2b14-4d96-b239-56f4bba62dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
